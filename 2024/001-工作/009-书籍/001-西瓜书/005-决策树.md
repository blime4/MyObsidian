决策树学习基本算法:
![[Pasted image 20240312104052.png]]

## 1. 划分选择
我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的“纯度” (purity) 越来越高.
1. [[信息增益]]
	1. [[信息熵]]
	2. 决策树生成步骤：
		1. 计算出根节点的信息熵
		2. 计算出其他属性的信息增益
			1. 如：![[Pasted image 20240312190024.png]]
			2. 纹理的信息增益最大
		3. 找出信息增益最大的，进行划分
		4. 重复 2-3
		5. 得到最终的决策树：
			1. 如：![[Pasted image 20240312190152.png]]
		6. 
	3. 缺点：
		1. 信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响
		2. 改进：使用 [[增益率]]
2. [[增益率]]
	1. 先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的.
3. [[基尼指数]]
## 2. 剪枝处理
1.  剪枝 (pruning) 是决策树学习算法对付“过拟合”的主要手段. 在
2. [[预剪枝]] (prepruning) 
	1. 预剪枝是指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点；
	2. 利用 [[验证集]]
3. [[后剪枝]] (postpruning)
	1. 后剪枝则是先从训练集生成一棵完整的决策树, 然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点带来决策树泛化性能提升，则将该子树替换为叶结点.
4. 
## 3. 连续与缺失值
## 4. 多变量与决策树
## 5. 参考资料

