决策树学习基本算法:
![[Pasted image 20240312104052.png]]

## 1. 划分选择
我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的“纯度” (purity) 越来越高.
1. [[信息增益]]
	1. [[信息熵]]
	2. 决策树生成步骤：
		1. 计算出根节点的信息熵
		2. 计算出其他属性的信息增益
			1. 如：![[Pasted image 20240312190024.png]]
			2. 纹理的信息增益最大
		3. 找出信息增益最大的，进行划分
		4. 重复 2-3
		5. 得到最终的决策树：
			1. 如：![[Pasted image 20240312190152.png]]
		6. 
	3. 缺点：
		1. 信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响
		2. 改进：使用 [[增益率]]
2. [[增益率]]
	1. 先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的.
3. [[基尼指数]]
## 2. 剪枝处理
1.  剪枝 (pruning) 是决策树学习算法对付“过拟合”的主要手段. 
2. ![[Pasted image 20240313113737.png]]
3. [[预剪枝]] (prepruning) 
	1. 预剪枝是指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点；
	2. 利用 [[验证集]] 进行调整，是否分支 | [[贪心]]算法
	3. ![[Pasted image 20240313113807.png]]
4. [[后剪枝]] (postpruning)
	1. 后剪枝则是先从训练集生成一棵完整的决策树, 然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点带来决策树泛化性能提升，则将该子树替换为叶结点.
	2. ![[Pasted image 20240313113910.png]]
5. 
## 3. 连续与缺失值
	由于连续属性的可取值数目不再有限，因此，不能直接根据连续属性的可 取值来对结点进行划分.止匕时，连续属性离散化技术可派上用场.最简单的策 略是采用二分法(bi-partition) 对连续属性进行处理
1. 连续属性处理

	![[Pasted image 20240313115109.png]]![[Pasted image 20240313115240.png]]
1. 缺失值处理
	1. ![[Pasted image 20240313115458.png]]
	2. 如何在属性值缺失的情况下进行划分属性选择?
	3. 给定划分属性，若样本在该属性上的值缺失, 如何对样本进行划分
	4. #未看完 
## 4. 多变量决策树
1. [[轴平行]]: 
	1. 决策树所形成的分类边界有一个明显的特点：轴平行 (axis-parallel)，即它的分类边界由若干个与坐标轴平行的分段组成
## 5. 参考资料

