
地址： https://www.bilibili.com/video/BV1X24y1A7Rz

## GPT Assistant training pipeline
![[Pasted image 20240105171411.png]]

## Pretraining
### Data collection
![[Pasted image 20240105171641.png]]
### Tokenization
![[Pasted image 20240105171733.png]]
### 2 example models
![[Pasted image 20240105172104.png]]
[[LLaMA]] > [[GPT3]] : you shouldn't judge the power of a model just by the number of parameters that it contains
### Pretraining inputs
![[Pasted image 20240105172843.png]]
### Step 1: pretaining Step 2: finetuning
![[Pasted image 20240105173950.png]]
### Base models in the wild
![[Pasted image 20240105180431.png]] 
> Currently the beat available base model probably is the llaMA series from Meta although it is not commercially licensed.

> Base models are NOT 'Assistants'

## Supervised Finetuning
