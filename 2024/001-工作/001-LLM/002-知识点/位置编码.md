前置知识：
1. [[线性代数]]
2. [[自注意力机制]]


[Transformer学习笔记一：Positional Encoding（位置编码） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/454482273)

为什么要有位置编码：
1. 因为 [[attention]] 序列中，不再有任何信息能够提示单词之间的相对位置关系
	1. 关键词：[[自注意力机制]]

对位置信息的感知能力
bag of words | 不具有
RNN               | 具有
Transfermer    | 具有

![[PosEnc.pdf]]
## 绝对位置编码
公式推导： https://www.bilibili.com/video/BV1ff4y1a7VL
## 相对位置编码
平移普遍性
1. [[XLNet]]
2. [[T5]]
3. [[DeBERTa]]
## 旋转位置编码
1. [[VIT]]



